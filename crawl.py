# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NGNLCb6eEd5p8OHDxaEwa8MxCITRoNn9
"""

!wget https://raw.githubusercontent.com/P4CSS/PSS/master/data/stopwords_zh-tw.txt -O stopwords_zh-tw.txt

!pip install jieba

import jieba
import jieba.analyse

!wget https://raw.githubusercontent.com/victorgau/wordcloud/master/SourceHanSansTW-Regular.otf

!pip install selenium
!apt-get update # to update ubuntu to correctly run apt install
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0,'/Users/user/Downloads/chromedriver')
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import requests
from bs4 import BeautifulSoup
import pandas as pd

import csv
with open('/content/7月文章資料 - 6_29-7_2.csv','r') as csvfile:
  reader = csv.DictReader(csvfile)
  for row in reader: 
    links = row["連結"]

    r = requests.get(links)
    web_content = r.text
    soup = BeautifulSoup(web_content,'lxml')

    articleContent = soup.find_all('p')
    articleContent

    '''article = []
    for p in articleContent:
      article.append(p.text)

    articleAll = '\n'.join(article)'''
    df = pd.read_csv('/content/7月文章資料 - 6_29-7_2.csv')
    df1= pd.concat([pd.DataFrame([p.text], columns=['內容']) for p in articleContent],
          ignore_index=True)
    
    print(df1)



import pandas as pd

with open("stopwords_zh-tw.txt", encoding="utf-8") as fin:
    stopwords = fin.read().split("\n")[1:]
    stopwords += ["\n"]


    df['token_text'] = df1['內容'].apply(lambda x:list(jieba.cut(x)))
    for i in stopwords:
      if i in df['token_text']:
        df['cleaned'] = df['token_text'].drop(stopwords)
        df['cleaned'] = df['cleaned'].apply(remove_punc_by_unicode)
      else:
        pass
    
        

        df.head(-50)

from collections import Counter
word_count = Counter()

# tokens_list=[]
# for tokens in df['token_text']:
#   print(len(tokens))

for tokens in df['token_text']:
    for tok in tokens:
        try:
            if len(tok) > 1 and not unicodedata.category(tok[0]).startswith('P'):
                word_count[tok] += 1
        except:
            print("%s\tTypeError: category() argument must be a unicode character, not str"%(tok))
for k, v in word_count.most_common(30):
    print(k, '\t', v)

word_pair_counts = Counter()

for tokens in df['token_text']:
    for tok in tokens:
      for i in range(len(tok) - 1):
        (w1, w2) = (tok[i], tok[i +1])
        word_pair_counts[(w1, w2)] += 1
print(word_pair_counts)




# for i in range(len(tokens) - 1):
#     (w1, w2) = (tokens[i], tokens[i + 1])
#     word_pair_counts[(w1, w2)] += 1

# print(word_pair_counts)